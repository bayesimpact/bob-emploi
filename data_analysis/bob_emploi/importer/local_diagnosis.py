# encoding: utf-8
"""Importer for local job data mapped to ROME job groups to MongoDB.

The data will be imported into the `local_diagnosis` collection and follows
the structure of LocalJobStats from job.proto.

The data from this importer is indexed by `departement_id` and `rome_id` and
contains bmo, salaries and unemployment_duration.

You can try it out on a local instance:
 - Start your local environment with `docker-compose up frontend-dev`.
 - Run this script:
    docker-compose run --rm data-analysis-prepare \
        python bob_emploi/importer/local_diagnosis.py \
        --bmo_csv data/bmo/bmo_2015.csv \
        --fap_rome_crosswalk data/crosswalks/passage_fap2009_romev3.txt \
        --salaries_csv data/fhs_salaries.csv \
        --unemployment_duration_csv data/fhs_category_a_duration.csv \
        --job_offers_changes_json data/job_offers/job_offers_changes.json \
        --job_imt_json data/scraped_imt_local_job_stats.json \
        --mongo_url mongodb://frontend-db/test
"""
import codecs
import json
import locale
import numpy
import pandas

from bob_emploi.lib import cleaned_data
from bob_emploi.lib import mongo
from bob_emploi.lib import read_data

# Average number of days per month.
_DAYS_PER_MONTH = 30.5
_QUANTILES = {'min': 0.35, 'median': 0.5, 'max': 0.65}
locale.setlocale(locale.LC_ALL, 'fr_FR.UTF-8')

# Minimum number of jobseekers that is needed before we compute statistics.
_MIN_SALARY_COUNT = 15

# TODO: Make the "UserWarning: Boolean Series.." disappear.


def _isnan(value):
    """Check whether a Python value is numpy's NaN."""
    return isinstance(value, float) and numpy.isnan(value)


def _namedtuple_to_json_dict(item, fields, int_fields):
    for field in fields:
        value = getattr(item, field)
        if _isnan(value):
            continue
        # Pandas' itertuples treats column names starting with underscore
        # specially.
        if field == 'local_id':
            field = '_id'
        if field in int_fields:
            value = int(value)
        yield field, value


def csv2dicts(
        bmo_csv, fap_rome_crosswalk, salaries_csv, unemployment_duration_csv,
        job_offers_changes_json, job_imt_json, mobility_csv, data_folder='data'):
    """Import departement level diagnosis data in MongoDB.

    Args:
      bmo_csv: path to a CSV file containing the BMO data.
      fap_rome_crosswalk: path to the passage file from FAP to ROME codes.
      unemployment_duration_csv: path to the file with unemployment period
        durations. Can be generated by `make data/fhs_category_a_duration.csv`.
      job_offers_change_json: path to the file with job offers changes. Can be
        generated by `make data/job_offers/job_offers_changes.json`.
      job_imt_json: path to the file scraped from the IMT website. Can be
        generated by `make data/scraped_imt_local_job_stats.json`.
      mobility_csv: path to the ROME CSV file containing mobility between jobs
        and job groups.
    """
    bmo_rome_data = _get_bmo_rome_data(bmo_csv, fap_rome_crosswalk)
    fhs_salaries = _get_fhs_salaries(salaries_csv)
    job_offers_changes = pandas.read_json(
        job_offers_changes_json, dtype={
            'jobOffersChange': int,
            'numJobOffersLastYear': int,
            'numJobOffersPreviousYear': int,
        })
    job_offers_changes.rename(columns={'_id': 'local_id'}, inplace=True)
    unemployment_durations = _get_unemployment_durations(
        unemployment_duration_csv)
    imt = _get_job_group_imt(job_imt_json)
    less_stressful = _get_less_stressful_job_groups(data_folder, mobility_csv, job_imt_json)
    local_diagnosis = pandas.merge(
        bmo_rome_data, fhs_salaries, on='local_id', how='outer')
    local_diagnosis = pandas.merge(
        local_diagnosis, unemployment_durations, on='local_id', how='outer')
    local_diagnosis = pandas.merge(
        local_diagnosis, job_offers_changes, on='local_id', how='outer')
    local_diagnosis = pandas.merge(
        local_diagnosis, imt, on='local_id', how='outer')
    local_diagnosis = pandas.merge(
        local_diagnosis, less_stressful, on='local_id', how='outer')
    int_columns = (set(job_offers_changes.columns) - set(['local_id']))
    return [
        dict(_namedtuple_to_json_dict(item, [
            'local_id', 'bmo', 'salary', 'imt', 'lessStressfulJobGroups',
            # TODO: Consider removing this one now that we have the more
            # precise one (per city).
            'unemploymentDuration',
            'jobOffersChange', 'numJobOffersLastYear',
            'numJobOffersPreviousYear'], int_columns))
        for item in local_diagnosis.itertuples()]


def _get_unemployment_durations(unemployment_duration_csv):
    """Get a very simple unemployment duration estimate from FHS data.

    We tried to built a more complex model but did not come to a satisfying
    solution. That's why we decided to simply compute the median for now.

    Args:
        unemployment_duration_csv: A file created via fhs_category_duration.py.
            See Makefile for examples of how to run it.

    Returns: A dataframe with `local_id` and one column with
        unemployment_duration objects that fit the DurationEstimation proto.
    """
    last_periods = pandas.read_csv(
        unemployment_duration_csv, dtype={'city_id': str})
    last_periods['departement_id'] = last_periods.city_id.str[:2]
    # Oversee départements are 3 digits long.
    last_periods.loc[last_periods.departement_id == '97', 'departement_id'] = (
        last_periods.city_id.str[:3])
    group_cols = ['code_rome', 'departement_id']
    unemployment_durations = last_periods.groupby(
        group_cols).duration.median().reset_index()
    local_id = unemployment_durations.departement_id.str.cat(
        unemployment_durations.code_rome, sep=':')
    return pandas.DataFrame({
        'unemploymentDuration': [
            {'days': int(item.duration)} for item in unemployment_durations.itertuples()],
        'local_id': local_id
    })


def _get_bmo_rome_data(bmo_csv, fap_rome_crosswalk):
    """Extract BMO information from a CSV.

    Currently this is pretty hacky: the BMO is defined for each
    "Bassin d'emploi" and each "Métier" as a FAP code, but our application
    relies on "département" and ROME code.

    We aggregate the data at the département level.

    HACK: Each FAP is mapped to a ROME ID, if a FAP maps to several ROME IDs
    we just pick one of them and if several FAPs map to the same ROME ID we
    just pick one of those.
    # TODO: Keep it as FAP and have our application find the user's FAP.

    Args:
      bmo_csv: path to a CSV file containing the BMO data.
      fap_rome_crosswalk: path to the passage file from FAP to ROME codes.
    """
    with codecs.open(fap_rome_crosswalk, 'r', 'latin-1') as fap_rome_file:
        fap_rome = read_data.parse_fap_rome_crosswalk(fap_rome_file.readlines())
    # As mentionned in the function's doc, this is not optimal.
    fap_rome.rome = fap_rome.rome.str[:5]
    fap_rome.drop_duplicates('fap', inplace=True)
    fap_to_rome = fap_rome.set_index('fap').rome

    bmo = pandas.read_csv(bmo_csv, dtype={
        'DEPARTEMENT_CODE': str,
        'REGION_CODE': str,
    }).fillna(0)
    bmo.rename(columns={
        'DEPARTEMENT_CODE': 'departement_id',
        'FAP_CODE': 'fap',
        'NB_RECRUT_PROJECTS': 'hiring_planned',
        'NB_SEASON_RECRUT_PROJECTS': 'seasonal_hiring_planned',
        'NB_DIFF_RECRUT_PROJECTS': 'difficult_hiring_planned',
    }, inplace=True)
    bmo['rome_id'] = bmo.fap.map(fap_to_rome)

    # Sum up data by département (because it's based on bassins d'emploi).
    # TODO: Get the user's bassin d'emploi and stop grouping it.
    bmo_rome = bmo.groupby(
        ['rome_id', 'departement_id'], sort=False, as_index=False).sum()
    bmo_rome['percentSeasonal'] = (
        bmo_rome.seasonal_hiring_planned * 100 /
        bmo_rome.hiring_planned).round().astype(int)
    bmo_rome['percentDifficult'] = (
        bmo_rome.difficult_hiring_planned * 100 /
        bmo_rome.hiring_planned).round().astype(int)
    return pandas.DataFrame({
        'bmo': [{
            'percentSeasonal': int(item.percentSeasonal),
            'percentDifficult': int(item.percentDifficult),
        } for item in bmo_rome.itertuples()],
        'local_id': bmo_rome.departement_id.str.cat(bmo_rome.rome_id, sep=':')
    })


def _get_fhs_salaries(salaries_csv):
    """Get salary estimates from FHS dataset.

    Args:
        salaries_csv: path to a CSV file prepared by the fhs_salaries script.

    Returns:
        A dataframe with a `local_id` added for joining to other datasets.
    """
    # See http://go/pe:notebooks/datasets/FHS_salaries.ipynb
    salaries = pandas.read_csv(salaries_csv, dtype={'departement_id': str})
    # TODO: Make a better filter or clean up the data.
    salaries = salaries[
        (salaries.salary_high > 1000) & (salaries.salary_high < 100000)]
    # TODO: Fallback on nation-wide stats.
    salaries_groups = salaries.groupby(
        ['departement_id', 'code_rome'], sort=False, group_keys=False)
    fhs_salaries = salaries_groups.apply(
        _salaries_diagnosis).dropna().to_frame(name='salary').reset_index()
    fhs_salaries['local_id'] = fhs_salaries.departement_id.str.cat(
        fhs_salaries.code_rome, sep=':')
    return fhs_salaries[['local_id', 'salary']]


def _salaries_diagnosis(salaries):
    total_count = salaries['count'].sum()
    if total_count < _MIN_SALARY_COUNT:
        return None
    cumulative_count = salaries.sort_values('salary_high')['count'].cumsum()
    estimation = {}
    for name, quantile in _QUANTILES.items():
        quantile_salaries = salaries[cumulative_count <= quantile * total_count]
        if len(quantile_salaries):
            salary = float(quantile_salaries.salary_high.max())
        else:
            min_salary = float(salaries.salary_low.min())
            max_salary = float(salaries.salary_high.max())
            # All the job seekers are in the same bucket and we lost the
            # precise info about what is the distribution. We just assume an
            # even distribution inside the whole range.
            salary = min_salary * (1 - quantile) + quantile * max_salary
        estimation['%sSalary' % name] = salary
    return finalize_salary_estimation(estimation)


def _get_less_stressful_job_groups(data_folder, mobility_csv, job_imt_json):
    mobility = cleaned_data.rome_job_groups_mobility(data_folder, filename=mobility_csv)
    imt = pandas.read_json(job_imt_json, orient='records')
    imt['code_rome'] = imt.job.apply(
        lambda job: job.get('jobGroup', {}).get('romeId'))
    imt['departement_id'] = imt.city.apply(lambda city: city.get('departementId'))
    imt['market_score'] = imt.yearlyAvgOffersPer10Openings.div(imt.yearlyAvgOffersDenominator)
    imt = imt.set_index(['code_rome', 'departement_id'])
    imt.dropna(subset=['market_score'], inplace=True)

    # Extend ROME mobility with IMT market scores.
    scored_mobility = mobility.merge(
        imt.market_score.reset_index(), left_on='source_rome_id', right_on='code_rome')
    scored_mobility = scored_mobility.join(
        imt,
        on=['target_rome_id', 'departement_id'],
        how='inner',
        lsuffix='_source')

    # Keep only the best reorientation.
    best_reorientation = scored_mobility.sort_values('market_score')\
        .drop_duplicates(subset=['code_rome', 'departement_id'], keep='first')
    best_reorientation = best_reorientation[
        best_reorientation.market_score >= 1.5 * best_reorientation.market_score_source]

    imt_valuable_columns = ['yearlyAvgOffersPer10Openings', 'yearlyAvgOffersDenominator']
    return pandas.DataFrame([
        {
            'local_id': '%s:%s' % (r.departement_id, r.code_rome),
            'lessStressfulJobGroups': [{
                'jobGroup': {'romeId': r.target_rome_id, 'name': r.target_rome_name},
                'localStats': {'imt': dict(_namedtuple_to_json_dict(
                    r, imt_valuable_columns, imt_valuable_columns))},
            }],
        }
        for unused_index, r in best_reorientation.iterrows()])


def _get_job_group_imt(job_imt_json):
    local_imts = {}
    with open(job_imt_json) as job_imt_file:
        for job_imt_str in job_imt_file:
            if not job_imt_str.startswith('{'):
                continue
            job_imt = json.loads(job_imt_str.rstrip().rstrip(','))
            # Pop on purpose: no need to keep the job in the diagnosis.
            job = job_imt.pop('job')
            # Pop on purpose: no need to keep the city in the diagnosis.
            city = job_imt.pop('city')
            local_id = '%s:%s' % (
                city['departementId'], job['jobGroup']['romeId'])
            # If multiple values for the same local ID, just override.
            local_imts[local_id] = job_imt
    return pandas.DataFrame([
        {'local_id': local_id, 'imt': imt}
        for local_id, imt in local_imts.items()])


def finalize_salary_estimation(estimation):
    """Finalize the data for a SalaryEstimation proto.

    Args:
        estimation: a dict with min/max/medianSalary. This dict will be
            modified.

    Returns:
        The input dict with additional fields to be displayed.
    """
    estimation['shortText'] = '%s - %s' % (
        locale.format('%d', estimation['minSalary'], grouping=True),
        locale.format('%d', estimation['maxSalary'], grouping=True))
    estimation['unit'] = 'ANNUAL_GROSS_SALARY'
    return estimation


if __name__ == "__main__":
    mongo.importer_main(csv2dicts, 'local_diagnosis')  # pragma: no cover
